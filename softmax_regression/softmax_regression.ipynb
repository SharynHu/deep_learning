{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "$$\\mathbf{o}=\\mathbf{W}\\mathbf{x}+\\mathbf{b}$$\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "#### Maximize the lig-likelihood\n",
    "Here we still want to maximize the likelihood of $P(\\mathbf{Y}|\\mathbf{X})$, which is equivalent to minimizing the negtive log-likelihood\n",
    "$$-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),$$\n",
    "    And for each pair of $<\\mathbf{y}, \\mathbf{\\hat y}>$ over $q$ classes, the negtive log-likelihood is\n",
    "$$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.$$\n",
    "\n",
    "So the total loss function is \n",
    "$$L(\\mathbf{Y}, \\mathbf{\\hat Y})=-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^q y_j \\log \\hat{y}_j$$\n",
    "\n",
    "#### Minimize the loss entropy loss\n",
    "For each pair of $<\\mathbf{y}, \\mathbf{\\hat y}>$ over $q$ classes, the cross entropy is\n",
    "$$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemantation from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset' from '../dlutils/dataset.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import sys\n",
    "sys.path.append(\"../dlutils\")\n",
    "import importlib\n",
    "import model\n",
    "import loss\n",
    "import train\n",
    "import dataset\n",
    "importlib.reload(model)\n",
    "importlib.reload(loss)\n",
    "importlib.reload(train)\n",
    "importlib.reload(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the dataset\n",
    "Here we are using the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_fashion_mnist_dataset\n",
    "# get Fashion-MNIST DataLoader\n",
    "batch_size = 256\n",
    "train_loader,test_loader=load_fashion_mnist_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve text labels\n",
    "Here we define a function to retrieve the corresponding text label given the numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',\n",
    "        'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SoftmaxRegression\n",
    "num_train_samples, height, width = list(train_loader.dataset[0][0].shape)\n",
    "in_features = height*width\n",
    "out_features = 10\n",
    "net = SoftmaxRegression(in_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import CrossEntropyLoss\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss 2.154412, training accuracy 0.591717,testing loss 2.255690, testing accuracy 0.582700\n",
      "epoch 1, training loss 1.665212, training accuracy 0.661667,testing loss 1.767846, testing accuracy 0.651300\n",
      "epoch 2, training loss 1.430462, training accuracy 0.692317,testing loss 1.534156, testing accuracy 0.681600\n",
      "epoch 3, training loss 1.296828, training accuracy 0.714367,testing loss 1.397760, testing accuracy 0.701000\n",
      "epoch 4, training loss 1.202236, training accuracy 0.729317,testing loss 1.302553, testing accuracy 0.717600\n",
      "epoch 5, training loss 1.142621, training accuracy 0.741267,testing loss 1.245808, testing accuracy 0.729000\n",
      "epoch 6, training loss 1.083060, training accuracy 0.748017,testing loss 1.184234, testing accuracy 0.734200\n",
      "epoch 7, training loss 1.043397, training accuracy 0.752117,testing loss 1.144947, testing accuracy 0.738800\n",
      "epoch 8, training loss 1.005169, training accuracy 0.759717,testing loss 1.106884, testing accuracy 0.746500\n",
      "epoch 9, training loss 0.973998, training accuracy 0.766517,testing loss 1.076151, testing accuracy 0.752100\n"
     ]
    }
   ],
   "source": [
    "from train import sgd, classifier_accuracy\n",
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.reset_weights()\n",
    "net.to(device)\n",
    "loss.to(device)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for X,y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat = net(X)\n",
    "        ce_loss = loss(y_hat, y).sum()\n",
    "        ce_loss.backward()\n",
    "        sgd(net.parameters(), lr, batch_size)\n",
    "    \n",
    "    #after each epoch, calculate traning loss, testing loss and accuracy\n",
    "    with torch.no_grad():\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        train_cp = 0\n",
    "        test_cp = 0\n",
    "        for X, y in train_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            train_loss += loss(y_hat,y).sum()\n",
    "            train_cp += classifier_accuracy(y_hat, y)\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        train_cp = train_cp/len(train_loader.dataset)\n",
    "            \n",
    "        for X, y in test_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            test_loss += loss(y_hat,y).sum()\n",
    "            test_cp += classifier_accuracy(y_hat,y)\n",
    "        test_loss = test_loss/len(test_loader.dataset)\n",
    "        test_cp = test_cp/len(test_loader.dataset)\n",
    "        print(f'epoch {i}, training loss {float(train_loss):f}, training accuracy {float(train_cp):f},testing loss {float(test_loss):f}, testing accuracy {float(test_cp):f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(in_features, out_features))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m)==torch.nn.Linear:\n",
    "        m.weight.data.zero_()\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss 53.603989, training accuracy 0.780850, testing loss 55.988068, testing accuracy 0.771900\n",
      "epoch 1, training loss 62.484596, training accuracy 0.792083, testing loss 65.545845, testing accuracy 0.784400\n",
      "epoch 2, training loss 74.655914, training accuracy 0.733033, testing loss 79.184387, testing accuracy 0.724700\n",
      "epoch 3, training loss 21.902069, training accuracy 0.831067, testing loss 24.407562, testing accuracy 0.816400\n",
      "epoch 4, training loss 19.011396, training accuracy 0.830017, testing loss 22.082993, testing accuracy 0.815100\n",
      "epoch 5, training loss 19.658064, training accuracy 0.842033, testing loss 22.473108, testing accuracy 0.828700\n",
      "epoch 6, training loss 22.639240, training accuracy 0.830367, testing loss 25.885845, testing accuracy 0.817200\n",
      "epoch 7, training loss 15.621553, training accuracy 0.851950, testing loss 18.589893, testing accuracy 0.831200\n",
      "epoch 8, training loss 15.852967, training accuracy 0.851700, testing loss 18.795355, testing accuracy 0.832400\n",
      "epoch 9, training loss 14.142015, training accuracy 0.857150, testing loss 17.053953, testing accuracy 0.837900\n"
     ]
    }
   ],
   "source": [
    "from train import train_3ch\n",
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "loss.to(device)\n",
    "train_3ch(net, loss, num_epochs, train_loader, trainer, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

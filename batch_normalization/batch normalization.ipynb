{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "1. **internal covariate shift**:change in the distribution of network activations due to change in the network parameters during training, which **slows down the traning by requiring lower training rates and careful parameter initialization**.\n",
    "2. The situation is worse when the network is deep, amplifying the internal covariate shift layer by layer and when the activations are saturating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Batch Normalization\n",
    "1. metigates internal covariate shift.\n",
    "2. allows us to use much higher training rates;\n",
    "3. allows us to be less careful about initialization and the use of activation functions.\n",
    "4. acts as a regularizer, in some cases eliminating the need for Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Reducing Internal Covariate Shift\n",
    "1. The network training converges faster if its inputs are whitened-i.e.,linearly transformed to have zero means and unit variances, and decorrelated.\n",
    "\n",
    "2. It would be advantageous to achieve the same whitening of the inputs of each layer.\n",
    "\n",
    "    Consider a network computing \n",
    "$$l=F_2(F_1(u,\\theta_1),\\theta_2)$$\n",
    "where $F_1$ and $F_2$ are arbitrary transformations, and the parameters $\\theta_1$, $\\theta_2$ are to be learnt so as to minimize the loss $l$. Learning $\\theta_2$ can be viewed as if the inputs $x=F_1(u, \\theta_1)$ are fed into the network\n",
    "$$l=F_2(x,\\theta_2).$$\n",
    "So if fixing the input distribution works for the whole network, it should also benefit the sub-network.\n",
    "\n",
    "3. The optimization step must take the normalization into account, or else parameter updates maybe eliminated by the normalization procedure.\n",
    "\n",
    "4. The full whitening of thelayer inputs is costly and not everywhere differentiable. So the authors made 2  necessary simpilification.\n",
    "    - Each scalar feature is normalized independently, by making it have the mean of zero and the variance of one. Such normalization speeds convergence even of the features are not decorrelated.\n",
    "    - use mini-batches to estimate the means and variance of each activation.\n",
    "\n",
    "5. Simply normalizing each input of a layer may change what the layer can represent, the authors introduced, for each activation, a pair of parameters to scale and shift the normalized value, enabling the normalizing transformation to represent an identity transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization Algorithm\n",
    "\n",
    "Formally, denoting by $\\mathbf{x} \\in \\mathcal{B}$ an input to batch normalization $BN$ that is from a minibatch $\\mathcal{B}$, batch normalization transforms  $\\mathbf{x}$ according to the following expression:\n",
    "$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "where, $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ is the sample mean and $\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}$ is the sample standard deviation of the minibatch $\\mathcal{B}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we calculate $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and $\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}$ as follows:\n",
    "$$\n",
    "\\begin{split}\\begin{aligned} \\hat{\\boldsymbol{\\mu}}_\\mathcal{B} &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x},\\\\\n",
    "\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.\\end{aligned}\\end{split}\n",
    "$$\n",
    "\n",
    "Note that we add a small constant $\\epsilon>0$ to the variance estimate to ensure that we never attempt division by zero, even in cases where the empirical variance estimate might vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization at testing\n",
    "Batch Normalization layer works differently at training time (normalizaed by batch statistics) and testing time (normalized by dataset statistics).\n",
    "\n",
    "There are 2 reasons:\n",
    "1. First, the noise in the sample mean and the sample variance arising from estimating each on minibatches are no longer desirable once we have trained the model. we don't want our prediction to be different just because of which batch it resides;\n",
    "2. we might not have the luxury of computing per-batch normalization statistics. For example, we might need to apply our model to make one prediction at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization for CNN\n",
    "For convolutional layers, all activations in a certain channel are normalized using the statistics of this channel, and all activations in this channel share the same shifting and scaling parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use `is_grad_enabled` to determine whether the current mode is training\n",
    "    # mode or prediction mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # If it is prediction mode, directly use the mean and variance\n",
    "        # obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully-connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of `X`, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=[0,2,3], keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dim=[0,2,3],keepdim=True)\n",
    "        # In training mode, the current mean and variance are used for the\n",
    "        # standardization\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # `num_features`: the number of outputs for a fully-connected layer\n",
    "    # or the number of output channels for a convolutional layer. `num_dims`:\n",
    "    # 2 for a fully-connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims==2:\n",
    "            # this is a fully-connected layer\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            # this is a convolutional layer\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # If `X` is not on the main memory, copy `moving_mean` and\n",
    "        # `moving_var` to the device where `X` is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated `moving_mean` and `moving_var`\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean, self.moving_var,\n",
    "            eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Batch Normalization in LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
    "                    nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16,\n",
    "                              kernel_size=5), BatchNorm(16, num_dims=4),\n",
    "                    nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Flatten(), nn.Linear(16 * 4 * 4, 120),\n",
    "                    BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "                    nn.Linear(120, 84), BatchNorm(84, num_dims=2),\n",
    "                    nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../dlutils\")\n",
    "from dataset import load_fashion_mnist_dataset\n",
    "\n",
    "batch_size = 128\n",
    "train_loader, test_loader = load_fashion_mnist_dataset(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss 0.004905, training accuracy 0.743550, testing loss 0.005218, testing accuracy 0.731900\n",
      "epoch 1, training loss 0.006141, training accuracy 0.702183, testing loss 0.006472, testing accuracy 0.697000\n",
      "epoch 2, training loss 0.003736, training accuracy 0.817383, testing loss 0.004050, testing accuracy 0.805300\n",
      "epoch 3, training loss 0.002724, training accuracy 0.872633, testing loss 0.003062, testing accuracy 0.859000\n",
      "epoch 4, training loss 0.002630, training accuracy 0.878100, testing loss 0.003032, testing accuracy 0.865700\n",
      "epoch 5, training loss 0.007594, training accuracy 0.669683, testing loss 0.008074, testing accuracy 0.658600\n",
      "epoch 6, training loss 0.002788, training accuracy 0.863167, testing loss 0.003240, testing accuracy 0.848100\n",
      "epoch 7, training loss 0.002742, training accuracy 0.872867, testing loss 0.003162, testing accuracy 0.860200\n",
      "epoch 8, training loss 0.002711, training accuracy 0.873067, testing loss 0.003213, testing accuracy 0.860300\n",
      "epoch 9, training loss 0.002061, training accuracy 0.902600, testing loss 0.002532, testing accuracy 0.886100\n"
     ]
    }
   ],
   "source": [
    "from train import train_3ch\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss.to(device)\n",
    "net.to(device)\n",
    "train_3ch(net, loss, num_epochs, train_loader, optimizer, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),\n",
    "                    nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16),\n",
    "                    nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Flatten(), nn.Linear(256, 120), nn.BatchNorm1d(120),\n",
    "                    nn.Sigmoid(), nn.Linear(120, 84), nn.BatchNorm1d(84),\n",
    "                    nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training loss 0.003689, training accuracy 0.827050, testing loss 0.003929, testing accuracy 0.815200\n",
      "epoch 1, training loss 0.005375, training accuracy 0.735267, testing loss 0.005571, testing accuracy 0.730400\n",
      "epoch 2, training loss 0.003833, training accuracy 0.802933, testing loss 0.004114, testing accuracy 0.793500\n",
      "epoch 3, training loss 0.003585, training accuracy 0.824850, testing loss 0.003918, testing accuracy 0.815700\n",
      "epoch 4, training loss 0.002935, training accuracy 0.858333, testing loss 0.003196, testing accuracy 0.846100\n",
      "epoch 5, training loss 0.002799, training accuracy 0.864117, testing loss 0.003153, testing accuracy 0.851600\n",
      "epoch 6, training loss 0.002951, training accuracy 0.846683, testing loss 0.003346, testing accuracy 0.827000\n",
      "epoch 7, training loss 0.002646, training accuracy 0.870883, testing loss 0.003009, testing accuracy 0.859100\n",
      "epoch 8, training loss 0.002545, training accuracy 0.876617, testing loss 0.002983, testing accuracy 0.863800\n",
      "epoch 9, training loss 0.002474, training accuracy 0.880333, testing loss 0.002936, testing accuracy 0.863200\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net2.parameters(), lr=1)\n",
    "train_3ch(net2, loss, num_epochs, train_loader, optimizer, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torchtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

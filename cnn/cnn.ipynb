{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import sys\n",
    "sys.path.append(\"../dlutils\")\n",
    "import importlib\n",
    "import model\n",
    "import dataset\n",
    "import train\n",
    "import loss\n",
    "importlib.reload(model)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(train)\n",
    "importlib.reload(loss)\n",
    "from dataset import load_fashion_mnist_dataset\n",
    "from train import train_3ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need CNN\n",
    "\n",
    "![](./cnn.png)\n",
    "1. to reserve the spacial information of the input;\n",
    "2. to use less parameters;\n",
    "3. to avoid overfitting caused by large amount of parameters.\n",
    "4. to get some translation invariance and locality;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross Correlation Operation\n",
    "Suppose we have an input of shape $n_h \\times n_w$, and a kernel of shape $k_h \\times k_w$, after doing the cross correlation operation, the output shape is given by:\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1)$$\n",
    "\n",
    "<img src=\"cross_correlation.png\" alt=\"Cross Correlation Operation\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[19., 25.],\n",
       "         [37., 43.]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Conv2D\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "net = Conv2D(K.shape)\n",
    "net.weight.data = K\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer for multiple input channels\n",
    "For a multi-channeled input, the kernel should have the same number of channels as it to do cross-correlation operation on each channel. The cross-correlation outputs will be added up to get the final output.\n",
    "\n",
    "Here is the illustration:\n",
    "\n",
    "<img src=\"conv-multi-in.svg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer for multiple input and output channels\n",
    "Denote by $c_i$ and $c_o$ the number of input and output channels, respectively, and let $k_h$ and $k_w$ be the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape $c_i \\times k_h \\times k_w$ for every output channel. We concatenate them on the output channel dimension, so that **the shape of the convolution kernel is $c_o \\times c_i \\times k_h \\times k_w$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1 \\times 1$ convolution layer\n",
    "Because the minimum window is used, the $1 \\times 1$ convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions.  **The only computation of the $1 \\times 1$ convolution occurs on the channel dimension.**\n",
    "\n",
    "You could think of the $1 \\times 1$ convolutional layer as constituting a **fully-connected layer applied at every single pixel location** to transform the $c_i$ corresponding input values into $c_o$ output values.\n",
    "\n",
    "One by one convolution helps us achieve:\n",
    "1. dimention reduction;\n",
    "2. feature transformation;\n",
    "3. deeper network; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Conv2DOnebyOne\n",
    "X = torch.rand(size=(5,4,4))\n",
    "net = Conv2DOnebyOne(5, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of  $p_w$  columns of padding (roughly half on the left and half on the right), the output shape will be\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "<img src=\"conv-pad.svg\">\n",
    "\n",
    "In many cases, we will want to set $p_h=k_h-1$  and $p_w = k_w-1$ to give the input and output the same height and width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "<img src=\"conv-stride.svg\">\n",
    "\n",
    "In general, when the stride for the height is $s_h$ and the stride for the width is  $s_w$ , the output shape is\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n",
    "**Explanation:**\n",
    "Take the horizontal row as an example. The total width of the row is $n_w+p_w$, and we know that for each small unit for cross-correlation operation, the starting index is divisible by $s_w$, and it should be smaller than $n_w+p_w-k_w$. So the total number of units should be $ \\lfloor(n_w+p_w-k_w)/s_w\\rfloor+1$, which is $\\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "The pooling layer, is used to reduce the spatial dimensions, but not depth, on a convolution neural network, model, basically this is what you gain:\n",
    "- By having less spatial information you gain computation performance\n",
    "- Less spatial information also means less parameters, so less chance to over-fit\n",
    "- You get some translation invariance\n",
    "\n",
    "Illustration:\n",
    "<img src=\"pooling.svg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode=\"max\"):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer with Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the defualt stide for maxpooling is kernel_size.\n",
    "pool2d = torch.nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = torch.nn.MaxPool2d((2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

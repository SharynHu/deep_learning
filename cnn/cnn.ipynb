{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import sys\n",
    "sys.path.append(\"../dlutils\")\n",
    "import importlib\n",
    "import model\n",
    "import dataset\n",
    "import train\n",
    "import loss\n",
    "importlib.reload(model)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(train)\n",
    "importlib.reload(loss)\n",
    "from dataset import load_fashion_mnist_dataset\n",
    "from train import train_3ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross Correlation Operation\n",
    "Suppose we have an input of shape $n_h \\times n_w$, and a kernel of shape $k_h \\times k_w$, after doing the cross correlation operation, the output shape is given by:\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1)$$\n",
    "\n",
    "<img src=\"cross_correlation.png\" alt=\"Cross Correlation Operation\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[19., 25.],\n",
       "         [37., 43.]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Conv2D\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "net = Conv2D(K.shape)\n",
    "net.weight.data = K\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer for multiple input channels\n",
    "For a multi-channeled input, the kernel should have the same number of channels as it to do cross-correlation operation on each channel. The cross-correlation outputs will be added up to get the final output.\n",
    "\n",
    "Here is the illustration:\n",
    "\n",
    "<img src=\"conv-multi-in.svg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer for multiple input and output channels\n",
    "Denote by $c_i$ and $c_o$ the number of input and output channels, respectively, and let $k_h$ and $k_w$ be the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape $c_i \\times k_h \\times k_w$ for every output channel. We concatenate them on the output channel dimension, so that **the shape of the convolution kernel is $c_o \\times c_i \\times k_h \\times k_w$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1 \\times 1$ convolution layer\n",
    "Because the minimum window is used, the $1 \\times 1$ convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions.  **The only computation of the $1 \\times 1$ convolution occurs on the channel dimension.**\n",
    "\n",
    "You could think of the $1 \\times 1$ convolutional layer as constituting a **fully-connected layer applied at every single pixel location** to transform the $c_i$ corresponding input values into $c_o$ output values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5076,  0.1259, -0.7726, -0.4522],\n",
      "         [-0.3095, -0.1037, -0.2447, -0.6873],\n",
      "         [-0.4602, -0.3451, -0.6279, -0.5538],\n",
      "         [-0.0835, -0.2740, -0.2498, -0.3815]],\n",
      "\n",
      "        [[-0.1522,  0.2037, -0.2166,  0.3916],\n",
      "         [-0.1256,  0.4036,  0.0974,  0.1388],\n",
      "         [ 0.2759,  0.4469,  0.2058,  0.1556],\n",
      "         [ 0.5209,  0.1955,  0.5365, -0.2528]],\n",
      "\n",
      "        [[-0.6011, -0.3757, -0.9705, -0.5223],\n",
      "         [-0.7133, -0.6213, -0.5570, -0.8425],\n",
      "         [-0.6292, -0.5921, -0.6994, -0.7364],\n",
      "         [-0.7720, -0.5177, -0.5744, -0.8175]],\n",
      "\n",
      "        [[-0.4625, -0.8049, -0.4650, -0.4133],\n",
      "         [-0.6588, -0.5933, -0.5655, -0.4099],\n",
      "         [-0.3993, -0.4365, -0.3109, -0.4797],\n",
      "         [-0.6188, -0.4864, -0.4540, -0.6795]],\n",
      "\n",
      "        [[-0.0365, -0.0435, -0.2070,  0.2668],\n",
      "         [-0.2685, -0.1204,  0.0051,  0.0048],\n",
      "         [ 0.0177,  0.2795,  0.1312,  0.1045],\n",
      "         [-0.0955, -0.1245,  0.1675, -0.4195]],\n",
      "\n",
      "        [[ 0.1964, -0.0450,  0.0950, -0.1164],\n",
      "         [ 0.0352, -0.3379,  0.0511, -0.0930],\n",
      "         [-0.1955, -0.0875, -0.0422,  0.0049],\n",
      "         [-0.3856, -0.2084, -0.2649,  0.0254]],\n",
      "\n",
      "        [[-0.1879, -0.1909,  0.1785,  0.1912],\n",
      "         [-0.0508,  0.2662, -0.0962,  0.3413],\n",
      "         [ 0.2362,  0.1931,  0.2024,  0.1890],\n",
      "         [ 0.4356,  0.0758,  0.2926, -0.0093]],\n",
      "\n",
      "        [[ 0.5740,  0.3099,  0.2744,  0.1249],\n",
      "         [ 0.3829,  0.3326,  0.5644,  0.1105],\n",
      "         [ 0.2823,  0.4394,  0.4462,  0.1812],\n",
      "         [ 0.3894,  0.3617,  0.3644,  0.3189]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "from model import Conv2DOnebyOne\n",
    "X = torch.rand(size=(5,4,4))\n",
    "net = Conv2DOnebyOne(5, 8)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of  $p_w$  columns of padding (roughly half on the left and half on the right), the output shape will be\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "<img src=\"conv-pad.svg\">\n",
    "\n",
    "In many cases, we will want to set $p_h=k_h-1$  and $p_w = k_w-1$ to give the input and output the same height and width. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride\n",
    "<img src=\"conv-stride.svg\">\n",
    "\n",
    "In general, when the stride for the height is $s_h$ and the stride for the width is  $s_w$ , the output shape is\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n",
    "**Explanation:**\n",
    "Take the horizontal row as an example. The total width of the row is $n_w+p_w$, and we know that for each small unit for cross-correlation operation, the starting index is divisible by $s_w$, and it should be smaller than $n_w+p_w-k_w$. So the total number of units should be $ \\lfloor(n_w+p_w-k_w)/s_w\\rfloor+1$, which is $\\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "The pooling layer, is used to reduce the spatial dimensions, but not depth, on a convolution neural network, model, basically this is what you gain:\n",
    "- By having less spatial information you gain computation performance\n",
    "- Less spatial information also means less parameters, so less chance to over-fit\n",
    "- You get some translation invariance\n",
    "\n",
    "Illustration:\n",
    "<img src=\"pooling.svg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode=\"max\"):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer with Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the defualt stide for maxpooling is kernel_size.\n",
    "pool2d = torch.nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = torch.nn.MaxPool2d((2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

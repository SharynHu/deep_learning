{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is bagging\n",
    "bagging is short for \"bootstrap aggregating\".\n",
    "It aims at producing an ensemble model that is more robust than the individual models composing it (reduce variance).\n",
    "\n",
    "Due to the theoretical variance of the training dataset (we remind that a dataset is an observed sample coming from a true unknown underlying distribution), the fitted model is also subject to variability: if another dataset had been observed, we would have obtained a different model.\n",
    "\n",
    "The idea of bagging is then simple: we want to **fit several independent models and “average” their predictions in order to obtain a model with a lower variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for bagging\n",
    "\n",
    "![](./bagging.png)\n",
    "1. create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution.\n",
    "2. fit a weak learner for each of these samples and finally aggregate them such that we kind of “average” their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to average the final output\n",
    "\n",
    "1. For a regression problem, the outputs of individual models can literally be averaged to obtain the output of the ensemble model. \n",
    "2. For classification problem the class outputted by each model can be seen as a vote and the class that receives the majority of the votes is returned by the ensemble model (this is called **hard-voting**). \n",
    "3. Still for a classification problem, we can also consider the probabilities of each classes returned by all the models, average these probabilities and keep the class with the highest average probability (this is called **soft-voting**). \n",
    "\n",
    "Averages or votes can either be simple or weighted if any relevant weights can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why bagging works?\n",
    "\n",
    "Roughly speaking, as the bootstrap samples are approximatively independent and identically distributed (i.i.d.), so are the learned base models. Then, “averaging” weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

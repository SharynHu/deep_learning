{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is boosting\n",
    "Unlike bagging that mainly aims at reducing variance, boosting is a technique that consists in fitting sequentially multiple weak learners in a very adaptative way: each model in the sequence is fitted **giving more importance to observations in the dataset that were badly handled by the previous models in the sequence.**\n",
    "\n",
    "Intuitively, each new model focus its efforts on the most difficult observations\n",
    " to fit up to now, so that we obtain, at the end of the process, **a strong learner with lower bias** (even if we can notice that boosting can also have the effect of reducing variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models chosen for boosting\n",
    "\n",
    "Being mainly focused at reducing bias, the base models that are often considered for boosting are **models with low variance but high bias**.  \n",
    "\n",
    "Another important reason that motivates the use of low variance but high bias models as weak learners for boosting is that **these models are in general less computationally expensive to fit** (few degrees of freedom when parameterised). Indeed, as computations to fit the different models canâ€™t be done in parallel (unlike bagging), it could become too expensive to fit sequentially several complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do boosting fit these weak learners?\n",
    "\n",
    "Most booosting algorithms train these weak learners by changing the distribution of the dataset, and fitting the altered dataset to the weak learners. \n",
    "\n",
    "So there are two basic questions for a boosting algorithm:\n",
    "1. how to change the distribution of a dataset?\n",
    "2. how to aggregate these waek learners to get a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost is short for \"adaptive boosting\". \n",
    "\n",
    "### Training steps for AdaBoost\n",
    "\n",
    "Suppose we are training binary classifier using **0-1** loss function $I$, where the training dataset is $\\{\\mathbf{x_i}, y_i\\}_{i=1}^{N}$, and we are training $M$ weak classifiers $f_m(x)\\in\\{-1, 1\\}$, then the psedocode for the Adaboost function is as follows:\n",
    "\n",
    "![](./adaboost_algorithm.png)\n",
    "\n",
    "So the steps can be summerized as:\n",
    "1. Initialize the weight for each training sample to $\\frac{1}{N}$;\n",
    "2. Train a weak classifier using the weighted **0-1** loss function and calculate the new weight for each weak classifier;\n",
    "3. Using the updated weights to train the next weak classifier until all $M$ weak classfiers are trained;\n",
    "\n",
    "### Aggregation steps for Adaboost\n",
    "\n",
    "After the $M$ weak classfiers are learnt, the final classfier is based on a linear combination of the weak classifiers:\n",
    "$$g(\\mathbf{x}) = sign\\left(\\sum_{m=1}^{M}\\alpha_m f_m(\\mathbf{x})\\right) $$\n",
    "\n",
    "### Note\n",
    "\n",
    "In AdaBoost, all weak classfiers are trained sequencially. For the final classifier, only weak learners with a 50% higher accuracy have positive weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1.  https://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/AdaBoost.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

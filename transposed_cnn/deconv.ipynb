{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transposed (Fractionally-strided) Convolutional Layers\n",
    "\n",
    "## Why transeposed convolution is needed\n",
    "\n",
    "For some tasks like semantic segmentation, we need the input size and the output size to be the same. \n",
    "\n",
    "Transposed convolutional layers are used to increase (upsample) the spatial dimensions of intermediate feature maps, so that we can get back the the origianal input size even if the input has already been downsampled by CNN layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deconvolution Operation\n",
    "\n",
    "Deconvolution with a $2 \\times 2$ input, a $2 \\times 2$ kernal, stride 1 and no padding:\n",
    "![](./trans_conv.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an input size of $n_h, n_w$, a kernel size of $k_h, k_w$. The final output size will be\n",
    "$$(n_h+k_h-1, n_w+k_w-1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_2d(X, K):\n",
    "    k_h, k_w = K.shape\n",
    "    n_h, n_w = X.shape\n",
    "    Y = torch.zeros(size = (n_h+k_h-1, n_w+k_w-1))\n",
    "    \n",
    "    for i in range(n_h):\n",
    "        for j in range(n_w):\n",
    "            Y[i:i+k_h, j:j+k_w] += K*X[i,j]\n",
    "            \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.],\n",
       "        [ 0.,  4.,  6.],\n",
       "        [ 4., 12.,  9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "deconv_2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 0.,  4.,  6.],\n",
       "          [ 4., 12.,  9.]]]], grad_fn=<SlowConvTranspose2DBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pytorch deconv API\n",
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "tconv = torch.nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconv with padding\n",
    "\n",
    "Deconv with padding is totally different from Conv with padding:\n",
    "1. it is applied to the output layer;\n",
    "2. it removes colomns and rows from the output layer.\n",
    "\n",
    "If the padding size is $(p_h, p_w)$, then $p_h$ rows will be removed both from the downside and upside of the output and $p_w$ rows will be removed both from the leftside and right side of the output. Thus the output size with padding $(p_h, p_w)$ will be \n",
    "$$(n_h+k_h-p_h*2-1, n_w+k_w-p_w*2-1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3., 2., 1.],\n",
      "          [2., 4., 6., 4., 2.],\n",
      "          [3., 6., 9., 6., 3.],\n",
      "          [2., 4., 6., 4., 2.],\n",
      "          [1., 2., 3., 2., 1.]]]], grad_fn=<SlowConvTranspose2DBackward>)\n",
      "tensor([[[[6., 9., 6.]]]], grad_fn=<SlowConvTranspose2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones((3, 3))\n",
    "K = torch.ones((3,3))\n",
    "X = X.reshape((1,1,3,3))\n",
    "K = K.reshape((1,1,3,3))\n",
    "tconv_no_padding = torch.nn.ConvTranspose2d(1, 1, kernel_size = 3, bias = False)\n",
    "tconv_no_padding.weight.data = K\n",
    "Y_no_padding = tconv_no_padding(X)\n",
    "print(Y_no_padding)\n",
    "tconv = torch.nn.ConvTranspose2d(1, 1, kernel_size = 3,  padding = (2, 1), bias = False)\n",
    "tconv.weight.data = K\n",
    "Y = tconv(X)\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconv with stride\n",
    "Different from Conv with stride, stride in Deconv is for intermediate result. Striding will make the output size larger.\n",
    "\n",
    "![](./trans_conv_stride2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the stride size is $s_h, s_w$. For every to adjacent element in a row, in the intermediate output, there will be a gap of size $s_h-1$, so the final output size in a row is $n_h+k_h-1+(s_h-1)*(n_h-1)$, which is $s_h*(n_h-1)+k_h$. \n",
    "\n",
    "The output size is \n",
    "$$(s_h \\times (n_h-1)+k_h, s_w \\times (n_w-1)+k_w).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconv for multiple channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple channels, like CNN, if the input has $c_i$ channels, then the kernel size will be $c_i \\times k_h \\times k_w$.  If the output channel size is $c_o$, then the kernel size will be $c_o \\times c_i \\times k_h \\times k_w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A trick\n",
    "An Deconv with the same parameters as a CNN will restore the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
